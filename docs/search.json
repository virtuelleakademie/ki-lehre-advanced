[
  {
    "objectID": "resources/resources.html",
    "href": "resources/resources.html",
    "title": "KI in der Lehre: Intermediate",
    "section": "",
    "text": "KI-basierte Schreibtools in der Lehre – Knowledge Base"
  },
  {
    "objectID": "resources/resources.html#ki-orientierungshilfe-der-bfh",
    "href": "resources/resources.html#ki-orientierungshilfe-der-bfh",
    "title": "KI in der Lehre: Intermediate",
    "section": "",
    "text": "KI-basierte Schreibtools in der Lehre – Knowledge Base"
  },
  {
    "objectID": "exercises/index.html",
    "href": "exercises/index.html",
    "title": "Übersicht",
    "section": "",
    "text": "Thema\n Dauer\n\n\n\n\nÜbung 1: Tutor erstellen\n10 min\n\n\nÜbung 2: Copilot Agenten erstellen\n25 min\n\n\nÜbung 3: Eigenen Agenten erstellen\n45 min\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Exercises",
      "Übersicht"
    ]
  },
  {
    "objectID": "exercises/exercise-2/index.html",
    "href": "exercises/exercise-2/index.html",
    "title": "Übung 2: Copilot Agenten erstellen",
    "section": "",
    "text": "Erstellen Sie einen MS Copilot Agenten mit Zugriff auf die Webseite der BFH Knowledge Base und instruieren Sie diesen zu didaktisch wertvollen Antworten, aber nur basierend auf Information in der Knowledge Base.\n\n\n\n\n\n\nURL der Knowledge Base\n\n\n\n\n\n https://virtuelleakademie.ch/knowledge-base/\n\n\n\n\n\n\n\n\n\nHinweis zur Verwendung\n\n\n\n\n\nSie können dem Agenten Webseiten zur Verfügung stellen. Per Webbrowser kann der Agent auf diese und untergeordnete Seiten zugreifen. Dann muss der Systemprompt so angepasst werden, dass die gegebene Antwort moeglichst korrekt und lehrreich ist.",
    "crumbs": [
      "Übungen",
      "Übung 2: Copilot Agenten erstellen"
    ]
  },
  {
    "objectID": "exercises/exercise-2/index.html#aufgabe",
    "href": "exercises/exercise-2/index.html#aufgabe",
    "title": "Übung 2: Copilot Agenten erstellen",
    "section": "",
    "text": "Erstellen Sie einen MS Copilot Agenten mit Zugriff auf die Webseite der BFH Knowledge Base und instruieren Sie diesen zu didaktisch wertvollen Antworten, aber nur basierend auf Information in der Knowledge Base.\n\n\n\n\n\n\nURL der Knowledge Base\n\n\n\n\n\n https://virtuelleakademie.ch/knowledge-base/\n\n\n\n\n\n\n\n\n\nHinweis zur Verwendung\n\n\n\n\n\nSie können dem Agenten Webseiten zur Verfügung stellen. Per Webbrowser kann der Agent auf diese und untergeordnete Seiten zugreifen. Dann muss der Systemprompt so angepasst werden, dass die gegebene Antwort moeglichst korrekt und lehrreich ist.",
    "crumbs": [
      "Übungen",
      "Übung 2: Copilot Agenten erstellen"
    ]
  },
  {
    "objectID": "exercises/exercise-2/index.html#beispielfragen-an-den-tutor",
    "href": "exercises/exercise-2/index.html#beispielfragen-an-den-tutor",
    "title": "Übung 2: Copilot Agenten erstellen",
    "section": "Beispielfragen an den Tutor",
    "text": "Beispielfragen an den Tutor\n\nWie kann ich mit dem Lernstick sicher prüfen?\nWelche Werkzeuge gibt es für die systematische Bewertung von mündlichen Präsentationen?\nWas ist mit dem Begriff Blended Learning gemeint?",
    "crumbs": [
      "Übungen",
      "Übung 2: Copilot Agenten erstellen"
    ]
  },
  {
    "objectID": "workshop/wie-chatbots-denken/index.html",
    "href": "workshop/wie-chatbots-denken/index.html",
    "title": "Wie Chatbots denken",
    "section": "",
    "text": "Diese Präsentation untersucht die Funktionsweise von Large Language Models (LLMs), ihre Unterschiede zu Chatbots, Assistenten und Agenten sowie deren Fähigkeit, kohärente, aber nicht immer genaue Antworten zu generieren.",
    "crumbs": [
      "Workshop",
      "Wie Chatbots denken"
    ]
  },
  {
    "objectID": "workshop/wie-chatbots-denken/index.html#präsentation",
    "href": "workshop/wie-chatbots-denken/index.html#präsentation",
    "title": "Wie Chatbots denken",
    "section": " Präsentation",
    "text": "Präsentation\n    View webpage in full screen",
    "crumbs": [
      "Workshop",
      "Wie Chatbots denken"
    ]
  },
  {
    "objectID": "workshop/prompting/index.html",
    "href": "workshop/prompting/index.html",
    "title": "Effective Prompting Strategies in Education",
    "section": "",
    "text": "Large Language Models (LLMs) can greatly enhance education by providing explanations, examples, and instant feedback. However, employing effective prompting techniques is critical. This affects whether LLMs support meaningful learning or potentially allow students to bypass learning altogether. Thoughtfully constructed prompts use principles from cognitive science, promoting active student engagement and deeper understanding.",
    "crumbs": [
      "Workshop",
      "Effective Prompting Strategies in Education"
    ]
  },
  {
    "objectID": "workshop/prompting/index.html#overview",
    "href": "workshop/prompting/index.html#overview",
    "title": "Effective Prompting Strategies in Education",
    "section": "",
    "text": "Large Language Models (LLMs) can greatly enhance education by providing explanations, examples, and instant feedback. However, employing effective prompting techniques is critical. This affects whether LLMs support meaningful learning or potentially allow students to bypass learning altogether. Thoughtfully constructed prompts use principles from cognitive science, promoting active student engagement and deeper understanding.",
    "crumbs": [
      "Workshop",
      "Effective Prompting Strategies in Education"
    ]
  },
  {
    "objectID": "workshop/prompting/index.html#key-principles-for-educational-prompting",
    "href": "workshop/prompting/index.html#key-principles-for-educational-prompting",
    "title": "Effective Prompting Strategies in Education",
    "section": "Key Principles for Educational Prompting",
    "text": "Key Principles for Educational Prompting\n\n1. Retrieval Practice\nEncourage recall of learned information to strengthen memory.\n\n\n\n\n\n\nTutor:\n\n\n\nQuiz me on three key points from the organic chemistry lecture on reaction kinetics [provided as PDF].\n\n\n\n\n2. Scaffolding\nBreak complex tasks into smaller steps to guide students gradually.\n\n\n\n\n\n\nTutor:\n\n\n\nFirst, give the balanced equation for this [chemical] reaction. Now, what are the initial concentrations?\n\n\n\n\n3. Metacognition\nPromote self-reflection and justification of reasoning.\n\n\n\n\n\n\nTutor:\n\n\n\nExplain why you chose this method for determining equilibrium. Are there assumptions you’ve made?\n\n\n\n\n4. Cognitive Load Management\nChunk information clearly to prevent overload.\n\n\n\n\n\n\nTutor:\n\n\n\nDefine entropy briefly. Next, explain how entropy differs from enthalpy.",
    "crumbs": [
      "Workshop",
      "Effective Prompting Strategies in Education"
    ]
  },
  {
    "objectID": "workshop/prompting/index.html#effective-prompting-techniques",
    "href": "workshop/prompting/index.html#effective-prompting-techniques",
    "title": "Effective Prompting Strategies in Education",
    "section": "Effective Prompting Techniques",
    "text": "Effective Prompting Techniques\n\nSet Clear Roles and Contexts\nProvide explicit roles to guide the LLM’s responses.\n\n\n\n\n\n\nExample Prompt:\n\n\n\nYou are an organic chemistry tutor helping a first-year student.\n\n\n\n\nSpecify Tasks and Formats Clearly\nBe specific to ensure precise responses.\n\n\n\n\n\n\nExample Prompt:\n\n\n\nExplain ionic bonding using a real-world analogy suitable for freshmen.\n\n\n\n\nUse Examples or Templates\nDemonstrate the desired output.\n\n\n\n\n\n\nExample Prompt:\n\n\n\nProvide a solution formatted as follows: First state the concept, then illustrate with a concrete chemistry example.\n\n\n\n\nChain-of-Thought and Reasoning\nAsk the LLM to detail its reasoning or provide multiple approaches.\n\n\n\n\n\n\nExample Prompt:\n\n\n\nStep-by-step, explain how to identify the limiting reagent in this reaction.\n\n\n\n\n\n\n\n\n✅\n\n\n\nInstruct the LLM to think first: “Explain your reasoning first, then state the answer.”\n\n\n\n\n\n\n\n\n❌\n\n\n\nInstruct the LLM to give the answer first: “State the answer first, then explain your reasoning.”\n\n\n\n\nIterative Refinement\nTreat prompting as an interactive process, refining outputs through conversation.\n\n\n\n\n\n\nExample Prompt:\n\n\n\nSimplify the previous explanation and provide a metaphor.\n\n\n\n\nUse Markdown Formatting\nUse Markdown formatting to make the prompt more readable (e.g. lists, bold, italics, etc.).\n\n\n\n\n\n\nBasic Markdown Formatting\n\n\n\n\n\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\n**Bold text**\n\n*Italic text*\n\n1. List item 1\n2. List item 2\n3. List item 3\nUse delimiters (e.g. ---, \"\"\") to indicate different roles or parts of a prompt.",
    "crumbs": [
      "Workshop",
      "Effective Prompting Strategies in Education"
    ]
  },
  {
    "objectID": "workshop/prompting/index.html#example-teaching-activities",
    "href": "workshop/prompting/index.html#example-teaching-activities",
    "title": "Effective Prompting Strategies in Education",
    "section": "Example Teaching Activities",
    "text": "Example Teaching Activities\n\nIllustrative Analogies\n\n\n\n\n\n\nExample Prompt:\n\n\n\nCreate an everyday analogy to illustrate Le Châtelier’s principle.\n\n\n\n\nPractice Questions Generation\n\n\n\n\n\n\nExample Prompt:\n\n\n\nCreate three practice questions on acid-base titrations at varying difficulty levels.\n\n\n\n\n\n\n\n\nNote that the task of generating practice questions is a complex task that requires a good understanding of the topic. It will be necessary to provide the LLM with a template for the questions, and to provide examples of how to format the questions. Additionally, you will need to consider how to define task difficulty very carefully.\n\n\n\n\n\nLesson Planning\n\n\n\n\n\n\nExample Prompt:\n\n\n\nOutline a 50-minute lesson plan on the ideal gas law with an interactive demonstration.\n\n\n\n\nInteractive Problem Solving\n\n\n\n\n\n\nExample Prompt:\n\n\n\nGuide me through solving a galvanic cell problem, providing hints without revealing the solution immediately.",
    "crumbs": [
      "Workshop",
      "Effective Prompting Strategies in Education"
    ]
  },
  {
    "objectID": "workshop/prompting/index.html#example-student-activities",
    "href": "workshop/prompting/index.html#example-student-activities",
    "title": "Effective Prompting Strategies in Education",
    "section": "Example Student Activities",
    "text": "Example Student Activities\n\nClarifying Concepts\n\n\n\n\n\n\nExample Prompt:\n\n\n\nSimplify and explain the concept of electrons behaving as waves.\n\n\n\n\nCreating Study Guides\n\n\n\n\n\n\nExample Prompt:\n\n\n\nSummarize thermodynamics laws and generate two review questions for each.\n\n\n\n\nSelf-Explanation and Reflection\n\n\n\n\n\n\nExample Prompt:\n\n\n\nEvaluate my explanation of buffer solutions and ask a clarifying follow-up question.\n\n\n\n\nError-Checking Practice\n\n\n\n\n\n\nExample Prompt:\n\n\n\nReview my solution to this equilibrium problem, identify mistakes, and guide me in correcting them.\n\n\n\n\nBrainstorming Project Ideas\n\n\n\n\n\n\nExample Prompt:\n\n\n\nSuggest three practical applications of electrochemistry suitable for a student project.",
    "crumbs": [
      "Workshop",
      "Effective Prompting Strategies in Education"
    ]
  },
  {
    "objectID": "workshop/chatbot-agenten/index.html",
    "href": "workshop/chatbot-agenten/index.html",
    "title": "Chatbot Agenten",
    "section": "",
    "text": "Diese Präsentation stellt KI-Chatbots als Analyse- und Lernwerkzeuge vor. Sie zeigt Beispiele wie zur Auswertung von Umfragen sowie die Sokratischen und Feynman-Tutoren, die durch gezielte Fragen und vereinfachte Erklärungen ein tieferes Verständnis fördern.",
    "crumbs": [
      "Workshop",
      "Chatbot Agenten"
    ]
  },
  {
    "objectID": "workshop/chatbot-agenten/index.html#präsentation",
    "href": "workshop/chatbot-agenten/index.html#präsentation",
    "title": "Chatbot Agenten",
    "section": " Präsentation",
    "text": "Präsentation\n    View webpage in full screen",
    "crumbs": [
      "Workshop",
      "Chatbot Agenten"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KI in der Lehre: Advanced",
    "section": "",
    "text": "25. April 2025\n9:00–12:00 Uhr\nRaum E103, Effingerstrasse 47, 3008 Bern"
  },
  {
    "objectID": "index.html#dozenten",
    "href": "index.html#dozenten",
    "title": "KI in der Lehre: Advanced",
    "section": "Dozenten",
    "text": "Dozenten\n\nDr. Andrew Ellis und Dr. Stefan Hackstein sind als wissenschaftliche Mitarbeiter an der Virtuellen Akademie der Berner Fachhochschule tätig. Zusammen erforschen sie, wie künstliche Intelligenz in Bildungssystemen wirkungsvoll eingesetzt werden kann."
  },
  {
    "objectID": "index.html#website",
    "href": "index.html#website",
    "title": "KI in der Lehre: Advanced",
    "section": "Website",
    "text": "Website\n virtuelleakademie.github.io/ki-lehre-advanced/"
  },
  {
    "objectID": "workshop/index.html",
    "href": "workshop/index.html",
    "title": "Ablauf",
    "section": "",
    "text": "Thema\n Dauer\n\n\n\n\nAuswertung der Umfrage\n10 min\n\n\nInput: Wie Chatbots denken\n25 min\n\n\nInput: Chatbot Agenten\n25 min\n\n\nEffective Prompting Strategies\nZum Selbststudium\n\n\nRAG slides\n10 min\n\n\nAPI Tricks\n10 min\n\n\nHowTo Colab\nZum Selbststudium\n\n\n\n\n\n\n Back to topReuseCC BY 4.0",
    "crumbs": [
      "Workshop",
      "Ablauf"
    ]
  },
  {
    "objectID": "workshop/umfrage/index.html",
    "href": "workshop/umfrage/index.html",
    "title": "Auswertung der Umfrage",
    "section": "",
    "text": "Wir werten die Umfrage mithilfe von ChatGPT aus (Live Demo).\n\n\n\n\n Back to topReuseCC BY 4.0",
    "crumbs": [
      "Workshop",
      "Auswertung der Umfrage"
    ]
  },
  {
    "objectID": "exercises/exercise-1/index.html",
    "href": "exercises/exercise-1/index.html",
    "title": "Übung 1: Tutor erstellen",
    "section": "",
    "text": "Passen Sie das gegebene Prompt-Template so an, dass der Chatbot in einen Tutor-Modus versetzt wird. Ziel ist es, dass der Chatbot didaktisch sinnvoll auf noch undefinierte Nutzereingaben reagiert und lernförderliches Verhalten zeigt.\n\n\n\n\n\n\nPrompt-Template\n\n\n\n\n\n# Lern-Tutor\n\nDu bist ein Tutor, der Nutzer dabei  unterstützt, ein tieferes Verständnis für ein Thema zu erlangen. \n\n---\n\n## Ziel\n\n*Der Nutzer soll ein echtes Verständnis für ein Thema entwickeln – nicht nur auswendig lernen, sondern nachvollziehen, wie und warum etwas funktioniert.*\n\n---\n\n## Verhaltensleitfaden\n\n### Wenn der Nutzer eine Frage stellt:\n- **[Platzhalter]**\n\n### Wenn der Nutzer eine Erklärung abgibt:\n- **[Platzhalter]**\n\n### Wenn der Nutzer eine Wissenslücke bemerkt:\n- **[Platzhalter]**\n\n### Wenn der Nutzer Fortschritte zeigt:\n- **[Platzhalter]**\n\n### Allgemein:\n- **[Platzhalter]**\n\n\n\n\n\n\n\n\n\n\nHinweis zur Verwendung\n\n\n\n\n\nDer Initial-Prompt wird einmalig zu Beginn eingegeben und dient dazu, den Chatbot in einen gewünschten Tutormodus zu versetzen. Er bildet die Grundlage für das Verhalten und die didaktische Interaktion im weiteren Verlauf. Wenn der Prompt weiter geändert werden soll, dann muss er direkt ganz oben im Chat editiert werden, oder mit dem geänderten Prompt ein neuer Chat gestartet werden.",
    "crumbs": [
      "Übungen",
      "Übung 1: Tutor erstellen"
    ]
  },
  {
    "objectID": "exercises/exercise-1/index.html#aufgabe",
    "href": "exercises/exercise-1/index.html#aufgabe",
    "title": "Übung 1: Tutor erstellen",
    "section": "",
    "text": "Passen Sie das gegebene Prompt-Template so an, dass der Chatbot in einen Tutor-Modus versetzt wird. Ziel ist es, dass der Chatbot didaktisch sinnvoll auf noch undefinierte Nutzereingaben reagiert und lernförderliches Verhalten zeigt.\n\n\n\n\n\n\nPrompt-Template\n\n\n\n\n\n# Lern-Tutor\n\nDu bist ein Tutor, der Nutzer dabei  unterstützt, ein tieferes Verständnis für ein Thema zu erlangen. \n\n---\n\n## Ziel\n\n*Der Nutzer soll ein echtes Verständnis für ein Thema entwickeln – nicht nur auswendig lernen, sondern nachvollziehen, wie und warum etwas funktioniert.*\n\n---\n\n## Verhaltensleitfaden\n\n### Wenn der Nutzer eine Frage stellt:\n- **[Platzhalter]**\n\n### Wenn der Nutzer eine Erklärung abgibt:\n- **[Platzhalter]**\n\n### Wenn der Nutzer eine Wissenslücke bemerkt:\n- **[Platzhalter]**\n\n### Wenn der Nutzer Fortschritte zeigt:\n- **[Platzhalter]**\n\n### Allgemein:\n- **[Platzhalter]**\n\n\n\n\n\n\n\n\n\n\nHinweis zur Verwendung\n\n\n\n\n\nDer Initial-Prompt wird einmalig zu Beginn eingegeben und dient dazu, den Chatbot in einen gewünschten Tutormodus zu versetzen. Er bildet die Grundlage für das Verhalten und die didaktische Interaktion im weiteren Verlauf. Wenn der Prompt weiter geändert werden soll, dann muss er direkt ganz oben im Chat editiert werden, oder mit dem geänderten Prompt ein neuer Chat gestartet werden.",
    "crumbs": [
      "Übungen",
      "Übung 1: Tutor erstellen"
    ]
  },
  {
    "objectID": "exercises/exercise-1/index.html#beispielfragen-an-den-tutor",
    "href": "exercises/exercise-1/index.html#beispielfragen-an-den-tutor",
    "title": "Übung 1: Tutor erstellen",
    "section": "Beispielfragen an den Tutor",
    "text": "Beispielfragen an den Tutor\n\nWie funktioniert ein Initial-Prompt?\nWas sind System und User Prompt und wo liegt der Unterschied?\nWie lernen LLMs?\nWie wählt ein LLM zufällig passende Worte?\nWährend das LLM eine Antwort schreibt, welchen Einfluss haben die schon geschriebenen Worte auf jene, die noch kommen?\nWelchen Einfluss hat die Struktur eines Prompts auf die Antwort des LLM?",
    "crumbs": [
      "Übungen",
      "Übung 1: Tutor erstellen"
    ]
  },
  {
    "objectID": "exercises/exercise-3/index.html",
    "href": "exercises/exercise-3/index.html",
    "title": "Übung 3: Eigenen Agenten erstellen",
    "section": "",
    "text": "Beispiel: Erstellen Sie einen KI-basierten Tutor, der ein pädagogisches Prinzip verkörpert und die Nutzer dabei unterstützt, ein tieferes Verständnis für ein Thema zu erlangen.\n\n\n\nÜberlegen Sie sich, wie Sie die Qualität Ihres Agenten gewährleisten können. Welche Kriterien sind wichtig?\n\n\n\n\n\n\nHinweis zur Verwendung\n\n\n\n\n\n\nVerwenden Sie entweder Copilot oder HuggingChat um Ihren Agenten zu erstellen.\nSie können mit den Prompt-Templates aus der Chatbot Agenten Präsentation beginnen und diese dann anpassen, oder Ihre eigene Idee umsetzen.\nÜberlegen Sie sich, wie Sie die Qualität Ihres Agenten testen (und verbessern) können.",
    "crumbs": [
      "Übungen",
      "Übung 3: Eigenen Agenten erstellen"
    ]
  },
  {
    "objectID": "exercises/exercise-3/index.html#aufgabe",
    "href": "exercises/exercise-3/index.html#aufgabe",
    "title": "Übung 3: Eigenen Agenten erstellen",
    "section": "",
    "text": "Beispiel: Erstellen Sie einen KI-basierten Tutor, der ein pädagogisches Prinzip verkörpert und die Nutzer dabei unterstützt, ein tieferes Verständnis für ein Thema zu erlangen.\n\n\n\nÜberlegen Sie sich, wie Sie die Qualität Ihres Agenten gewährleisten können. Welche Kriterien sind wichtig?\n\n\n\n\n\n\nHinweis zur Verwendung\n\n\n\n\n\n\nVerwenden Sie entweder Copilot oder HuggingChat um Ihren Agenten zu erstellen.\nSie können mit den Prompt-Templates aus der Chatbot Agenten Präsentation beginnen und diese dann anpassen, oder Ihre eigene Idee umsetzen.\nÜberlegen Sie sich, wie Sie die Qualität Ihres Agenten testen (und verbessern) können.",
    "crumbs": [
      "Übungen",
      "Übung 3: Eigenen Agenten erstellen"
    ]
  },
  {
    "objectID": "exercises/miro-board/index.html",
    "href": "exercises/miro-board/index.html",
    "title": "Miro Board",
    "section": "",
    "text": "View Miro board in new tab\n\n\n``\n\n\n\n Back to top",
    "crumbs": [
      "Übungen",
      "Miro Board"
    ]
  },
  {
    "objectID": "workshop/API-tricks/MoE.html",
    "href": "workshop/API-tricks/MoE.html",
    "title": "API Tricks",
    "section": "",
    "text": "Ein Prompt - mehrere anfragen\n\nSchreibe eine Antwort\nPrüfe auf Korrektheit\nPrüfe auf Richtlinien\n…\n\n\n\n\n\n\n\n\nimport openai\n\nopenai.api_key = \"sk-...\"\n\n# Schritt 1: Anfrage & Richtlinien\nuser_input = \"Wie viele Monde hat der Jupiter?\"\nrichtlinien = \"Antworten enthalten nur Fakten, keine Spekulation.\"\n\n\n# Schritt 2: Antwort generieren\nanswer = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\n        \"role\": f\"Antworte korrekt innerhalb der Richtlinien.\\n Richtlinien: {richtlinier}\",\n        \"content\": user_input}]\n).choices[0].message[\"content\"]\n\n# Schritt 3: Antwort validieren\ncorrect = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[{\n        \"role\": \"Prüfe auf Korrektheit. Antworte nur 'OK' wenn alles korrekt ist.\",\n        \"content\":\n            f\"Prüfe auf Korrektheit:\\n\"\n            f\"Frage: {user_input}\\nAntwort: {answer}\\n\"\n    }]\n).choices[0].message[\"content\"]\n\nproper = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[{\n        \"role\": \"Prüfe auf Richtlinien. Antworte nur 'OK' wenn alles korrekt ist.\",\n        \"content\":\n            f\"Prüfe auf Richtlinien:\\n\"\n            f\"Frage: {user_input}\\n Antwort: {answer}\\n Richtlinien: {richtlinien}\"\n    }]\n).choices[0].message[\"content\"]\n\n\n\n# Schritt 4: Ausgabe\nif not correct == \"OK\":\n    print(\"⚠️ Antwort ist inhaltlich falsch.\")\nelif not proper == \"OK\":\n    print(\"⛔ Verstoß gegen Richtlinien.\")\nelse:\n    print(answer)"
  },
  {
    "objectID": "workshop/API-tricks/MoE.html#moe-mixture-of-experts",
    "href": "workshop/API-tricks/MoE.html#moe-mixture-of-experts",
    "title": "API Tricks",
    "section": "",
    "text": "Ein Prompt - mehrere anfragen\n\nSchreibe eine Antwort\nPrüfe auf Korrektheit\nPrüfe auf Richtlinien\n…\n\n\n\n\n\n\n\n\nimport openai\n\nopenai.api_key = \"sk-...\"\n\n# Schritt 1: Anfrage & Richtlinien\nuser_input = \"Wie viele Monde hat der Jupiter?\"\nrichtlinien = \"Antworten enthalten nur Fakten, keine Spekulation.\"\n\n\n# Schritt 2: Antwort generieren\nanswer = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\n        \"role\": f\"Antworte korrekt innerhalb der Richtlinien.\\n Richtlinien: {richtlinier}\",\n        \"content\": user_input}]\n).choices[0].message[\"content\"]\n\n# Schritt 3: Antwort validieren\ncorrect = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[{\n        \"role\": \"Prüfe auf Korrektheit. Antworte nur 'OK' wenn alles korrekt ist.\",\n        \"content\":\n            f\"Prüfe auf Korrektheit:\\n\"\n            f\"Frage: {user_input}\\nAntwort: {answer}\\n\"\n    }]\n).choices[0].message[\"content\"]\n\nproper = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[{\n        \"role\": \"Prüfe auf Richtlinien. Antworte nur 'OK' wenn alles korrekt ist.\",\n        \"content\":\n            f\"Prüfe auf Richtlinien:\\n\"\n            f\"Frage: {user_input}\\n Antwort: {answer}\\n Richtlinien: {richtlinien}\"\n    }]\n).choices[0].message[\"content\"]\n\n\n\n# Schritt 4: Ausgabe\nif not correct == \"OK\":\n    print(\"⚠️ Antwort ist inhaltlich falsch.\")\nelif not proper == \"OK\":\n    print(\"⛔ Verstoß gegen Richtlinien.\")\nelse:\n    print(answer)"
  },
  {
    "objectID": "workshop/API-tricks/MoE.html#minimal-openai-mixture-of-experts-pipeline",
    "href": "workshop/API-tricks/MoE.html#minimal-openai-mixture-of-experts-pipeline",
    "title": "MoE: Mixture of Experts",
    "section": "⚙️ Minimal: OpenAI Mixture-of-Experts Pipeline",
    "text": "⚙️ Minimal: OpenAI Mixture-of-Experts Pipeline\nimport openai\n\nopenai.api_key = \"sk-...\"\n\n# Schritt 1: Anfrage & Richtlinien\nuser_input = \"Wie viele Monde hat der Jupiter?\"\nrichtlinien = \"Antworten enthalten nur Fakten, keine Spekulation.\"\n\n\n# Schritt 2: Antwort generieren\nanswer = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\n        \"role\": f\"Antworte korrekt innerhalb der Richtlinien.\\n Richtlinien: {richtlinier}\",\n        \"content\": user_input}]\n).choices[0].message[\"content\"]\n\n# Schritt 3: Antwort validieren\ncorrect = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[{\n        \"role\": \"Prüfe auf Korrektheit. Antworte nur 'OK' wenn alles korrekt ist.\",\n        \"content\":\n            f\"Prüfe auf Korrektheit:\\n\"\n            f\"Frage: {user_input}\\nAntwort: {answer}\\n\"\n    }]\n).choices[0].message[\"content\"]\n\nproper = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[{\n        \"role\": \"Prüfe auf Richtlinien. Antworte nur 'OK' wenn alles korrekt ist.\",\n        \"content\":\n            f\"Prüfe auf Richtlinien:\\n\"\n            f\"Frage: {user_input}\\n Antwort: {answer}\\n Richtlinien: {richtlinien}\"\n    }]\n).choices[0].message[\"content\"]\n\n\n\n# Schritt 4: Ausgabe\nif not correct == \"OK\":\n    print(\"⚠️ Antwort ist inhaltlich falsch.\")\nelif not proper == \"OK\":\n    print(\"⛔ Verstoß gegen Richtlinien.\")\nelse:\n    print(answer)"
  },
  {
    "objectID": "workshop/api-tricks/api-tricks.html",
    "href": "workshop/api-tricks/api-tricks.html",
    "title": "API Tricks",
    "section": "",
    "text": "Ein Chatbot ist mehr als eine einfache Anfrage an ein LLM. Vielmehr triggert jede Userprompt eine vielzahl von Anfragen, einerseits um die Antwort zu generieren, andererseits um die Qualität sicherzustellen.\n\n\nEin Prompt - mehrere anfragen\n\nSchreibe eine Antwort\nPrüfe auf Korrektheit\nPrüfe auf Richtlinien\n…\n\n\n\n\n\n\n\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nopenai.api_key = \"sk-...\"\n\n# Schritt 1: Anfrage & Richtlinien\nuser_input = \"Wie viele Monde hat der Jupiter?\"\nrichtlinien = \"Antworten enthalten nur Fakten, keine Spekulation.\"\n\n\n# Schritt 2: Antwort generieren\nanswer = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": f\"Antworte korrekt innerhalb der Richtlinien.\\n Richtlinien: {richtlinier}\"},\n        {\"role\": \"user\",\"content\": user_input}\n        ]\n).choices[0].message.content\n\n# Schritt 3: Antwort validieren\ncorrect = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[\n        {\"role\": \"system\", \"content\": \"Prüfe auf Korrektheit. Antworte nur 'OK' wenn alles korrekt ist.\"},\n        {\n            \"role\": \"user\", \n            \"content\":\n                f\"Prüfe auf Korrektheit:\\n\"\n                f\"Frage: {user_input}\\nAntwort: {answer}\\n\"\n        }\n    ]\n).choices[0].message.content\n\nproper = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[\n        {\"role\": \"system\", \"content\": \"Prüfe auf Richtlinien. Antworte nur 'OK' wenn alles korrekt ist.\"},\n        {\n            \"role\": \"user\", \n            \"content\":\n                f\"Prüfe auf Richtlinien:\\n\"\n                f\"Frage: {user_input}\\n Antwort: {answer}\\n Richtlinien: {richtlinien}\"\n    }]\n).choices[0].message.content\n\n\n\n# Schritt 4: Ausgabe\nif not correct == \"OK\":\n    print(\"⚠️ Antwort ist inhaltlich falsch.\")\nelif not proper == \"OK\":\n    print(\"⛔ Verstoß gegen Richtlinien.\")\nelse:\n    print(answer)"
  },
  {
    "objectID": "workshop/api-tricks/api-tricks.html#moe-mixture-of-experts",
    "href": "workshop/api-tricks/api-tricks.html#moe-mixture-of-experts",
    "title": "API Tricks",
    "section": "",
    "text": "Ein Chatbot ist mehr als eine einfache Anfrage an ein LLM. Vielmehr triggert jede Userprompt eine vielzahl von Anfragen, einerseits um die Antwort zu generieren, andererseits um die Qualität sicherzustellen.\n\n\nEin Prompt - mehrere anfragen\n\nSchreibe eine Antwort\nPrüfe auf Korrektheit\nPrüfe auf Richtlinien\n…\n\n\n\n\n\n\n\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nopenai.api_key = \"sk-...\"\n\n# Schritt 1: Anfrage & Richtlinien\nuser_input = \"Wie viele Monde hat der Jupiter?\"\nrichtlinien = \"Antworten enthalten nur Fakten, keine Spekulation.\"\n\n\n# Schritt 2: Antwort generieren\nanswer = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": f\"Antworte korrekt innerhalb der Richtlinien.\\n Richtlinien: {richtlinier}\"},\n        {\"role\": \"user\",\"content\": user_input}\n        ]\n).choices[0].message.content\n\n# Schritt 3: Antwort validieren\ncorrect = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[\n        {\"role\": \"system\", \"content\": \"Prüfe auf Korrektheit. Antworte nur 'OK' wenn alles korrekt ist.\"},\n        {\n            \"role\": \"user\", \n            \"content\":\n                f\"Prüfe auf Korrektheit:\\n\"\n                f\"Frage: {user_input}\\nAntwort: {answer}\\n\"\n        }\n    ]\n).choices[0].message.content\n\nproper = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",  ## leichtere Aufgabe -&gt; kleineres Modell\n    messages=[\n        {\"role\": \"system\", \"content\": \"Prüfe auf Richtlinien. Antworte nur 'OK' wenn alles korrekt ist.\"},\n        {\n            \"role\": \"user\", \n            \"content\":\n                f\"Prüfe auf Richtlinien:\\n\"\n                f\"Frage: {user_input}\\n Antwort: {answer}\\n Richtlinien: {richtlinien}\"\n    }]\n).choices[0].message.content\n\n\n\n# Schritt 4: Ausgabe\nif not correct == \"OK\":\n    print(\"⚠️ Antwort ist inhaltlich falsch.\")\nelif not proper == \"OK\":\n    print(\"⛔ Verstoß gegen Richtlinien.\")\nelse:\n    print(answer)"
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html",
    "href": "workshop/howto-colab/howto-colab.html",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "",
    "text": "In diesem Dokument zeigen wir, wie man ein Google Colab Notebook verwendet, um Anfragen an die OpenAI API zu stellen. Dies ist besonders nützlich für einfache Experimente mit Sprachmodellen wie GPT."
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#einführung",
    "href": "workshop/howto-colab/howto-colab.html#einführung",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "",
    "text": "In diesem Dokument zeigen wir, wie man ein Google Colab Notebook verwendet, um Anfragen an die OpenAI API zu stellen. Dies ist besonders nützlich für einfache Experimente mit Sprachmodellen wie GPT."
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#voraussetzungen",
    "href": "workshop/howto-colab/howto-colab.html#voraussetzungen",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Voraussetzungen",
    "text": "Voraussetzungen\nBevor du startest, benötigst du:\n\nEin kostenloses Google Konto.\nEinen OpenAI API Key"
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#schritt-1-google-colab-notebook-vorbereiten",
    "href": "workshop/howto-colab/howto-colab.html#schritt-1-google-colab-notebook-vorbereiten",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Schritt 1: Google Colab Notebook vorbereiten",
    "text": "Schritt 1: Google Colab Notebook vorbereiten\nÖffne ein neues Notebook in Google Colab und installiere das OpenAI-Paket:\n!pip install openai"
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#schritt-2-api-key-setzen",
    "href": "workshop/howto-colab/howto-colab.html#schritt-2-api-key-setzen",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Schritt 2: API Key setzen",
    "text": "Schritt 2: API Key setzen\nDu kannst den API-Schlüssel direkt im Notebook setzen (nicht empfohlen für öffentlich geteilte Notebooks) oder sicher über Umgebungsvariablen:\nimport openai\nopenai.api_key = \"DEIN_API_KEY_HIER\"\nAlternativ (sicherer):\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"DEIN_API_KEY_HIER\"\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")"
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#schritt-3-einfache-anfrage-an-gpt-3.5",
    "href": "workshop/howto-colab/howto-colab.html#schritt-3-einfache-anfrage-an-gpt-3.5",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Schritt 3: Einfache Anfrage an GPT-3.5",
    "text": "Schritt 3: Einfache Anfrage an GPT-3.5\n\nfrom openai import OpenAI\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Erkläre mir den code für eine OpenAI API ChatCompletion in einfachen Worten.\"}\n    ]\n)\n\nprint(response.choices[0].message.content)"
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#hinweise",
    "href": "workshop/howto-colab/howto-colab.html#hinweise",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Hinweise",
    "text": "Hinweise\n\nDie API ist kostenpflichtig. Prüfe deine Nutzung regelmäßig im OpenAI-Dashboard."
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#weiterführende-links",
    "href": "workshop/howto-colab/howto-colab.html#weiterführende-links",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Weiterführende Links",
    "text": "Weiterführende Links\n\nOpenAI Python API Doku\nGoogle Colab Einführung"
  },
  {
    "objectID": "workshop/api-tricks/api-tricks.html#durchdachte-antworten-zusammenfassen",
    "href": "workshop/api-tricks/api-tricks.html#durchdachte-antworten-zusammenfassen",
    "title": "API Tricks",
    "section": "Durchdachte Antworten zusammenfassen",
    "text": "Durchdachte Antworten zusammenfassen\nIm obigen Beispiel soll die Antwort nur “OK” lauten. Effektiv bringt eine solche Anfrage das Sprachmodell dazu zu wuerfeln, denn ein Denkprozess wird nur dann immitiert, wenn er auch verbalisiert wird. Ein langer Denkprozess kann auf eine kurze Antwort reduziert werden mittels eines zweiten API Calls.\n\nMinimal: Chain-of-Thought + Structured Summary\nimport openai\n\nopenai.api_key = \"sk-...\"\n\nuser_input = \"Schwimmt Eis auf Wasser?\"\n\n# Schritt 1: CoT-Antwort erzeugen\ncot_response = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\n        {\"role\": \"system\", \"content\":\"Finde Schritt für Schritt eine Antwort auf die Anfrage.\"},\n        {\"role\": \"user\", \"content\": user_input}\n    }]\n).choices[0].message.content\n\n# Schritt 2: Antwort minimal Zusammenfassen (Ja/Nein)\nclass IsCorrect(BaseModel):\n    answer_correct: bool\n\nsummary = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    response_format=IsCorrect ## Antwort wird eine Instanz der Klasse sein\n    messages=[\n        {\"role\": \"system\", \"content\": \"Gib nur die finale Antwort wieder.\"},\n        {\"role\": \"user\", \"content\": cot_response},\n\n    ]\n).choices[0].message.content\n\nlog.write(cot)  ## Denkprozess speichern zur Analyse\n\n# Ausgabe\nif check.choices[0].message.parsed.answer_correct:\n    print(\"✅ Die Antwort ist: Ja.\")\nelse:\n    print(\"❌ Die Antwort ist: Nein.\")"
  },
  {
    "objectID": "workshop/api-tricks/api-tricks.html#minimal-chain-of-thought-zusammenfassung",
    "href": "workshop/api-tricks/api-tricks.html#minimal-chain-of-thought-zusammenfassung",
    "title": "API Tricks",
    "section": "Minimal: Chain-of-Thought + Zusammenfassung",
    "text": "Minimal: Chain-of-Thought + Zusammenfassung\nimport openai\n\nopenai.api_key = \"sk-...\"\n\nuser_input = \"Schwimmt Eis auf Wasser?\"\n\n# Schritt 1: CoT-Antwort erzeugen\ncot_response = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[{\n        \"role\": \"Finde Schritt für Schritt eine Antwort auf die Anfrage.\",\n        \"content\": user_input\n    }]\n).choices[0].message[\"content\"]\n\n# Schritt 2: Antwort minimal Zusammenfassen (Ja/Nein)\nclass IsCorrect(BaseModel):\n    answer_correct: bool\n\nsummary = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    response_format=IsCorrect\n    messages=[{\n        \"role\": \"Gib nur die finale Antwort wieder.\",\n        \"content\": cot_response,\n\n    }]\n).choices[0].message[\"content\"]"
  },
  {
    "objectID": "workshop/howto-colab/howto-colab.html#schritt-2-api-key-mit-google-colab-secrets-setzen",
    "href": "workshop/howto-colab/howto-colab.html#schritt-2-api-key-mit-google-colab-secrets-setzen",
    "title": "OpenAI API Nutzung mit Google Colab",
    "section": "Schritt 2: API Key mit Google Colab Secrets setzen",
    "text": "Schritt 2: API Key mit Google Colab Secrets setzen\nUm deinen OpenAI API-Schlüssel sicher zu verwenden, empfehlen wir die Nutzung von Google Colab Secrets.\n\nKlicke links auf den Schlüssel\nAdd new secret\nFüge dort deinen API Key unter dem Namen OPENAI_API_KEY hinzu.\n\nDann kannst du im Notebook folgenden Code verwenden:\nfrom google.colab import userdata\nuserdata.get('OPENAI_API_KEY')```"
  },
  {
    "objectID": "workshop/api-tricks/api-tricks.html#mehrere-antworten-erhalten",
    "href": "workshop/api-tricks/api-tricks.html#mehrere-antworten-erhalten",
    "title": "API Tricks",
    "section": "Mehrere Antworten erhalten",
    "text": "Mehrere Antworten erhalten\nSprachmodelle neigen zum Halluzinieren. Gluecklicherweise sind diese Halluzinationen selten einheitlich. wenn wir eine Frage mehrmals beantworten lassen und jedes mal kommt das selbe heraus, steigt das die Wahrscheinlichkeit das es wirklich korrekt ist.\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"What are some creative icebreaker questions?\"}],\n    n=3  # Get 3 completions\n)\n\nfor choice in response.choices:\n    print(choice.message.content)"
  },
  {
    "objectID": "workshop/api-tricks/api-tricks.html#api-assistenten",
    "href": "workshop/api-tricks/api-tricks.html#api-assistenten",
    "title": "API Tricks",
    "section": "API Assistenten",
    "text": "API Assistenten\nOpenAI erlaubt API Assistenten zu definieren (analog zu CostumGPT), um sie mit minimalem Aufwand in verschiedenen Codes zu verwenden. Diese kommen mit einem fertigen RAG system, können mit einem code interpreter selsbgeschriebenen Pythoncode ausführen und erlauben Einstellung von Parametern und Responsformat."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Tutorials"
    ]
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "KI in der Lehre: Advanced",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "tutorials/structured-output/index.html",
    "href": "tutorials/structured-output/index.html",
    "title": "Structured Output",
    "section": "",
    "text": "A very useful feature of OpenAI’s API is the ability to return structured data. This is useful for a variety of reasons, but one of the most common is to return a JSON object. Here is the official OpenAI documentation for structured output.\nOpenAI’s API can return responses in structured formats like JSON, making it easier to:\nWhen using structured output, you can:\nCommon use cases include:\nPut very simply, the difference between structured and unstructured output is illustrated by the following example: Imagine you want to know the current weather in a city.\nUnstructured output: The response is a free-form text response.\nor\nStructured output: The response is a JSON object with the weather information.\nThe benefit of structured output is that it is easier to parse and process programmatically. A further advantage is that we can use a data validation library like Pydantic to ensure that the response is in the expected format.\nTo use this feature, we first need to install the pydantic package.\nThen we can define a Pydantic model to describe the expected structure of the response.\nWe can use this object as the response_format parameter in the parse method.",
    "crumbs": [
      "Tutorials",
      "Structured Output"
    ]
  },
  {
    "objectID": "tutorials/structured-output/index.html#extracting-facts-from-text",
    "href": "tutorials/structured-output/index.html#extracting-facts-from-text",
    "title": "Structured Output",
    "section": "Extracting facts from text",
    "text": "Extracting facts from text\nHere is an example of how to use structured output. Since a pre-trained model is not actually able to provide weather information without calling a weather API, we will use a prompt that asks the model to give us some facts contained in a text about a composer. For example, we want to extract the composer’s name, the year of birth and death, and the country of origin, the genre of music they worked in, and some key works.\n\nfrom dotenv import load_dotenv\nfrom openai import OpenAI \n\n\nload_dotenv()\n\nclient = OpenAI()\n\nNext we define a Pydantic model to describe the expected structure of the response. The fields of the model correspond to the facts we want to extract.\nIn this case, we want to extract the following facts (if available):\n\nThe composer’s name\nThe year of birth\nThe year of death\nThe country of origin\nThe genre of music they worked in\nSome key works\n\n\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\nclass ComposerFactSheet(BaseModel):\n    name: str\n    birth_year: int\n    death_year: Optional[int] = None  # Optional for living composers\n    country: str\n    genre: str\n    key_works: List[str]\n\nThis is a Pydantic model that defines a structured data format for storing information about composers:\n\nclass ComposerFactSheet(BaseModel): Creates a new class that inherits from Pydantic’s BaseModel, giving it data validation capabilities.\nname: str: A required field for the composer’s name.\nbirth_year: int: A required field for the year of birth.\ndeath_year: Optional[int] = None: An optional field for the year of death.\ncountry: str: A required field for the country of origin.\ngenre: str: A required field for the genre of music.\nkey_works: List[str]: A required field for a list of key works.\n\nWhen used, this model will:\n\nValidate that all required fields are present\nConvert input data to the correct types when possible\nRaise validation errors if data doesn’t match the schema\n\nExample output:\ncomposer = ComposerFactSheet(\n    name=\"Johann Sebastian Bach\",\n    birth_year=1685,\n    death_year=1750,\n    country=\"Germany\",\n    genre=\"Baroque\",\n    key_works=[\"Mass in B minor\", \"The Well-Tempered Clavier\"]\n)\nLet’s try this with a suitable system prompt and a short paragraph about Eric Satie. We will use the GPT-4o model for this.\n\ntext = \"\"\"\nÉric Alfred Leslie Satie (1866–1925) was a French composer and pianist known for his eccentric personality and groundbreaking contributions to music. Often associated with the Parisian avant-garde, Satie coined the term “furniture music” (musique d’ameublement) to describe background music intended to blend into the environment, an early precursor to ambient music. He is perhaps best known for his piano compositions, particularly the Gymnopédies and Gnossiennes, which are characterized by their simplicity, haunting melodies, and innovative use of harmony. Satie’s collaborations with artists like Claude Debussy, Pablo Picasso, and Jean Cocteau established him as a central figure in early 20th-century modernism. Despite his whimsical demeanor, he significantly influenced composers such as John Cage and minimalists of the mid-20th century.\n\"\"\"\n\n\nsystem_prompt = \"\"\"\nYou are an expert at extracting structured data from unstructured text.\n\"\"\"\n\nuser_message = f\"\"\"\nPlease extract the following information from the text: {text}\n\"\"\"\n\nThe f-string (formatted string literal)is used to embed the text variable into the user_message string. This allows us to dynamically construct the prompt that will be sent to the language model, including the specific text we want it to extract structured information from. Without the f-string, we would need to concatenate the strings manually, which can be more error-prone and less readable.\n\ncompletion = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \n        \"content\": system_prompt},\n        {\"role\": \"user\", \n        \"content\": user_message}\n    ],\n1    response_format=ComposerFactSheet\n)\n\n\n1\n\nresponse_format=ComposerFactSheet is the key line here. It tells the model to return a response in the format of the ComposerFactSheet model.\n\n\n\n\n\nfactsheet = completion.choices[0].message.parsed\nprint(factsheet)\n\nname='Éric Alfred Leslie Satie' birth_year=1866 death_year=1925 country='France' genre='Classical, Avant-garde' key_works=['Gymnopédies', 'Gnossiennes']\n\n\nWe can now access the fields of the factsheet object.\n\nfactsheet.name\n\n'Éric Alfred Leslie Satie'\n\n\n\nfactsheet.key_works\n\n['Gymnopédies', 'Gnossiennes']\n\n\nLet’s try another example. This time we will attempt to extract information from a paragraph in which some of the information is missing.\n\ntext_2 = \"\"\"\nFrédéric Chopin (1810) was a composer and virtuoso pianist, renowned for his deeply expressive and technically innovative piano works. Often called the “Poet of the Piano,” Chopin’s music, including his nocturnes, mazurkas, and polonaises, is celebrated for blending Polish folk elements with Romantic lyricism. Born near Warsaw, he spent much of his career in Paris, influencing generations of musicians and cementing his place as one of the greatest composers of all time.\n\"\"\"\n\n\nuser_message = f\"\"\"\nPlease extract the following information from the text: {text_2}\n\"\"\"\n\n\n\ncompletion_2 = client.beta.chat.completions.parse(\n    model=\"gpt-4o-2024-08-06\",\n    messages=[\n        {\"role\": \"system\", \n        \"content\": system_prompt},\n        {\"role\": \"user\", \n        \"content\": user_message}\n    ],\n    response_format=ComposerFactSheet\n)\n\n\ncompletion_2.choices[0].message.parsed\n\nComposerFactSheet(name='Frédéric Chopin', birth_year=1810, death_year=1849, country='Poland', genre='Classical - Romantic', key_works=['Nocturnes', 'Mazurkas', 'Polonaises'])\n\n\nAn obvious next step would be to improve our prompting strategy, so that the model indicates which fields it is able to fill in, and which fields are associated with uncertain or missing information.",
    "crumbs": [
      "Tutorials",
      "Structured Output"
    ]
  },
  {
    "objectID": "tutorials/structured-output/index.html#creating-a-reusable-function",
    "href": "tutorials/structured-output/index.html#creating-a-reusable-function",
    "title": "Structured Output",
    "section": "Creating a reusable function",
    "text": "Creating a reusable function\nHowever, we will focus on making our code more resuable by creating a function that can be called with different texts.\n\ndef extract_composer_facts(text: str) -&gt; ComposerFactSheet:\n    system_prompt = \"\"\"\n    You are an expert at extracting structured data from unstructured text.\n    \"\"\"\n\n    user_message = f\"\"\"\n    Please extract the following information from the text: {text}\n    \"\"\"\n    completion = client.beta.chat.completions.parse(\n        model=\"gpt-4.1\",\n        messages=[\n            {\"role\": \"system\", \n            \"content\": system_prompt},\n            {\"role\": \"user\", \n            \"content\": user_message}\n        ],\n        response_format=ComposerFactSheet\n    )\n    return completion.choices[0].message.parsed\n\n\nbach_text = \"\"\"\nJohann Sebastian Bach (1685–1750) was a German composer and musician of the Baroque era, widely regarded as one of the greatest composers in Western music history. His masterful works, including the Brandenburg Concertos, The Well-Tempered Clavier, and the Mass in B Minor, showcase unparalleled contrapuntal skill and emotional depth. Bach’s music has influenced countless composers and remains a cornerstone of classical music education and performance worldwide.\n\"\"\"\n\n\n\nextract_composer_facts(bach_text)\n\nComposerFactSheet(name='Johann Sebastian Bach', birth_year=1685, death_year=1750, country='Germany', genre='Baroque', key_works=['Brandenburg Concertos', 'The Well-Tempered Clavier', 'Mass in B Minor'])",
    "crumbs": [
      "Tutorials",
      "Structured Output"
    ]
  },
  {
    "objectID": "slides/openai-platform/index.html#openai-playground",
    "href": "slides/openai-platform/index.html#openai-playground",
    "title": "Using the OpenAI Platform",
    "section": "OpenAI Playground",
    "text": "OpenAI Playground"
  },
  {
    "objectID": "slides/openai-platform/index.html#generate-prompt",
    "href": "slides/openai-platform/index.html#generate-prompt",
    "title": "Using the OpenAI Platform",
    "section": "Generate Prompt",
    "text": "Generate Prompt"
  },
  {
    "objectID": "slides/openai-platform/index.html#system-prompt",
    "href": "slides/openai-platform/index.html#system-prompt",
    "title": "Using the OpenAI Platform",
    "section": "System Prompt",
    "text": "System Prompt"
  },
  {
    "objectID": "slides/openai-platform/index.html#llm-parameters",
    "href": "slides/openai-platform/index.html#llm-parameters",
    "title": "Using the OpenAI Platform",
    "section": "LLM Parameters",
    "text": "LLM Parameters"
  },
  {
    "objectID": "slides/openai-platform/index.html#generate-response",
    "href": "slides/openai-platform/index.html#generate-response",
    "title": "Using the OpenAI Platform",
    "section": "Generate Response",
    "text": "Generate Response"
  },
  {
    "objectID": "slides/openai-platform/index.html#view-code",
    "href": "slides/openai-platform/index.html#view-code",
    "title": "Using the OpenAI Platform",
    "section": "View Code",
    "text": "View Code"
  }
]